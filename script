import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Sklearn imports
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import (OneHotEncoder, OrdinalEncoder, StandardScaler)
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import (train_test_split, cross_val_score, 
                                     RandomizedSearchCV, GridSearchCV, KFold)
from sklearn.linear_model import LogisticRegression, LinearRegression, SGDRegressor
from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, 
                              GradientBoostingClassifier, BaggingClassifier, 
                              RandomForestRegressor)
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.svm import LinearSVC, SVR
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.metrics import (ConfusionMatrixDisplay, f1_score, 
                             make_scorer, confusion_matrix, 
                             mean_squared_error, mean_absolute_error)
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

from xgboost import XGBClassifier, XGBRegressor
from catboost import CatBoostClassifier

from scipy.stats import randint, uniform


# LOADING DATA

raw_df = pd.read_csv('car_insurance_claim.csv')
pd.set_option('display.max_columns', None)

# Quick peek
display(raw_df.head())

# Check shape and duplicates
print("Initial shape:", raw_df.shape)
duplicates_count = raw_df.duplicated().sum()
print("Duplicates:", duplicates_count)

# Drop duplicates
raw_df.drop_duplicates(inplace=True)
print("Shape after dropping duplicates:", raw_df.shape)

#RENAMING & BASIC COLUMN CLEANUP

column_map = {
    'KIDSDRIV': 'num_young_drivers',
    'BIRTH': 'date_of_birth',
    'AGE': 'age',
    'HOMEKIDS': 'num_of_children',
    'YOJ': 'years_job_held_for',
    'INCOME': 'income',
    'PARENT1': 'single_parent',
    'HOME_VAL': 'value_of_home',
    'MSTATUS': 'married',
    'GENDER': 'gender',
    'EDUCATION': 'highest_education',
    'OCCUPATION': 'occupation',
    'TRAVTIME': 'commute_dist',
    'CAR_USE': 'type_of_use',
    'BLUEBOOK': 'vehicle_value',
    'TIF': 'policy_tenure',
    'CAR_TYPE': 'vehicle_type',
    'RED_CAR': 'red_vehicle',
    'OLDCLAIM': 'fiveyr_claims_val',
    'CLM_FREQ': 'fiveyr_claims_num',
    'REVOKED': 'licence_revoked',
    'MVR_PTS': 'license_points',
    'CLM_AMT': 'new_claim_value',
    'CAR_AGE': 'vehicle_age',
    'CLAIM_FLAG': 'is_claim',
    'URBANICITY': 'address_type'
}

data_df = raw_df.copy()
data_df.rename(columns=column_map, inplace=True)

# If ID/date_of_birth exist, remove them
for drop_col in ['ID', 'date_of_birth']:
    if drop_col in data_df.columns:
        data_df.drop(drop_col, axis=1, inplace=True)

print("\nAfter renaming/dropping columns, columns are now:")
print(data_df.columns)

currency_cols = [
    'income', 'value_of_home', 
    'vehicle_value', 'fiveyr_claims_val', 
    'new_claim_value'
]

# Example function to strip currency formatting
def clean_currency_values(df, cols):
    
    for c in cols:
        df[c] = df[c].replace('[\\$,]', '', regex=True).astype('Int64')
    return df

data_df = clean_currency_values(data_df, currency_cols)

# If any columns have 'z_' prefix, remove them
z_prefix_cols = [
    'married', 'gender', 'highest_education', 
    'occupation', 'vehicle_type', 'address_type'
]

def strip_z_prefix(df, cols):
    
    for c in cols:
        df[c] = df[c].replace('[z_]', '', regex=True)
    return df

data_df = strip_z_prefix(data_df, z_prefix_cols)

print("\nPreview of dataset after cleaning:")
display(data_df.head())

# Convert new_claim_value to numeric; drop rows if it's missing
data_df['new_claim_value'] = pd.to_numeric(data_df['new_claim_value'], errors='coerce')
data_df.dropna(subset=['new_claim_value'], inplace=True)

# Binning claim values (linear bins)
bins_linear = [0.0, 5000, 10000, 15000, 20000, 
               25000, 30000, 35000, 40000, 
               45000, 50000, np.inf]
labels_linear = np.arange(1, len(bins_linear))
data_df['claim_value_cat'] = pd.cut(
    data_df['new_claim_value'], 
    bins=bins_linear, 
    labels=labels_linear, 
    include_lowest=True
)

# Binning claim values (log-scale) for EDA
log_bins = np.logspace(
    0, 
    np.log10(data_df['new_claim_value'].max()), 
    num=12
)
log_labels = [f"Bin {i}" for i in range(1, len(log_bins))]
data_df['log_claim_value_cat'] = pd.cut(
    data_df['new_claim_value'], 
    bins=log_bins, 
    labels=log_labels, 
    include_lowest=True
)

X_all = data_df.copy()
y_all = data_df['is_claim']

# drop the target columns from X
X_all.drop(columns=['new_claim_value', 'is_claim'], inplace=True)

# Basic split
X_train, X_test, y_train, y_test = train_test_split(
    X_all, 
    y_all, 
    test_size=0.2, 
    random_state=42,
    stratify=X_all['claim_value_cat'] 
)

# DATA INSPECTION: CORRELATION / QUICK EDA

# Example correlation check on the training set
eda_df = X_train.copy()
eda_df['is_claim'] = y_train

print("\nCorrelation with 'is_claim':")
corr_vals = eda_df.corr(numeric_only=True)['is_claim'].sort_values(ascending=False)
print(corr_vals)

# IMPUTATION EXAMPLE (KNN FOR NUMERIC, SIMPLE FOR CATEGORICAL)

X_train_raw = X_train.copy()

# 1) KNN Imputer for numeric columns
knn_imputer = KNNImputer(n_neighbors=2)
numeric_cols_list = X_train_raw.select_dtypes(include=[np.number]).columns.tolist()

def knn_impute_numeric(df, numeric_cols, imputer):
    
    sub_df = df[numeric_cols]
    imputed_arr = imputer.fit_transform(sub_df)
    imputed_df = pd.DataFrame(imputed_arr, columns=numeric_cols)
    return imputed_df

X_num_imputed = knn_impute_numeric(X_train_raw, numeric_cols_list, knn_imputer)

# 2) Simple Imputer for categorical columns
cat_cols_list = X_train_raw.select_dtypes(include=['object']).columns.tolist()
sim_imputer = SimpleImputer(strategy='most_frequent')

def simple_impute_categorical(df, cat_cols, imputer):
    
    sub_df = df[cat_cols]
    cat_imputed_arr = imputer.fit_transform(sub_df)
    cat_imputed_df = pd.DataFrame(cat_imputed_arr, columns=cat_cols)
    return cat_imputed_df

X_cat_imputed = simple_impute_categorical(X_train_raw, cat_cols_list, sim_imputer)

# Re-assemble numeric + categorical
X_train_imputed = pd.concat([X_num_imputed, X_cat_imputed], axis=1)

# ENCODING EXAMPLE (ORDINAL, BINARY, ONE-HOT)

# columns:
ord_cols_example = ['highest_education']
ord_cats_example = [['<High School','High School','Bachelors','Masters','PhD']]

bin_cols_example = ['single_parent','married','gender','type_of_use',
                    'licence_revoked','address_type']

onehot_cols_example = ['occupation','vehicle_type']

# Fit each encoder
oe = OrdinalEncoder(categories=ord_cats_example)
be = OrdinalEncoder()
ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

encoded_ord = oe.fit_transform(X_cat_imputed[ord_cols_example])
encoded_bin = be.fit_transform(X_cat_imputed[bin_cols_example])
encoded_ohe = ohe.fit_transform(X_cat_imputed[onehot_cols_example])

# Convert arrays to DataFrame
df_ord = pd.DataFrame(encoded_ord, columns=ord_cols_example)
df_bin = pd.DataFrame(encoded_bin, columns=bin_cols_example)
df_ohe = pd.DataFrame(encoded_ohe, columns=ohe.get_feature_names_out(onehot_cols_example))

# Combine everything
X_train_encoded = pd.concat([
    X_num_imputed.reset_index(drop=True), 
    df_ord.reset_index(drop=True),
    df_bin.reset_index(drop=True),
    df_ohe.reset_index(drop=True)
], axis=1)

# VIF CHECK 

def compute_vif(df):
    
    cdf = add_constant(df)
    vif_data = []
    for i in range(cdf.shape[1]):
        vif_data.append(variance_inflation_factor(cdf.values, i))
    return pd.DataFrame({'Feature': cdf.columns, 'VIF': vif_data})

vif_result = compute_vif(X_train_encoded)
display(vif_result)

# MODEL COMPARISON (CLASSIFIERS)
clf_candidates = [
    ('LogisticReg', LogisticRegression(solver='liblinear', max_iter=2000)),
    ('KNeighbors', KNeighborsClassifier()),
    ('DecisionTree', DecisionTreeClassifier()),
    ('RandForest', RandomForestClassifier(random_state=42)),
    ('LinearSVM', LinearSVC(random_state=42, dual='auto')),
    ('XGBoost', XGBClassifier(random_state=42)),
    ('AdaBoost', AdaBoostClassifier(random_state=42, algorithm='SAMME')),
    ('GradBoost', GradientBoostingClassifier(random_state=42)),
    ('Bagging', BaggingClassifier(random_state=42)),
    ('CatBoost', CatBoostClassifier(random_state=42, verbose=0))
]

cv_kf = KFold(n_splits=10, shuffle=True, random_state=42)

results_dict = {}
for name, model in clf_candidates:
    scores = cross_val_score(model, X_train_encoded, y_train, cv=cv_kf)
    results_dict[name] = scores

results_df = pd.DataFrame(results_dict)
fig, ax = plt.subplots(figsize=(14, 8))
sns.boxplot(data=results_df, ax=ax)
ax.set_xlabel("Classifier", fontsize=12)
ax.set_ylabel("CV Accuracy", fontsize=12)
ax.set_title("Accuracy Distribution (10-fold CV) for Various Models", fontsize=14)
plt.show()



